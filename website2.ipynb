{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool,  FactorRange\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Spectral10\n",
    "from bokeh.models import Legend, LegendItem\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "vector_embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "df = pd.read_pickle('./datasets/all_data_768dim_embeddings_fixed_authors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_visualization(clustered_df, query, vector_embedding_model):\n",
    "    # Create UMAP reducer for visualization\n",
    "    for_visual_umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1,  metric='euclidean', n_components=2, random_state=42)\n",
    "    \n",
    "    # Get embeddings for both documents and query\n",
    "    doc_embeddings = clustered_df['embedding_llm_768_dim'].tolist()\n",
    "    query_embedding = [vector_embedding_model.encode(query)]\n",
    "    all_embeddings = doc_embeddings + query_embedding\n",
    "    \n",
    "    # Transform all embeddings together\n",
    "    all_umap_embeddings = for_visual_umap_reducer.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Split back into document and query embeddings\n",
    "    for_visual_umap_embeddings = all_umap_embeddings[:-1]\n",
    "    query_umap_embedding = all_umap_embeddings[-1]\n",
    "\n",
    "    # Prepare the data for Bokeh\n",
    "    clustered_df['cluster'] = clustered_df['cluster'].astype(str)\n",
    "    unique_clusters = sorted(clustered_df['cluster'].unique())\n",
    "    colors = ['black' if cluster == '-1' else Spectral10[i % len(Spectral10)] for i, cluster in enumerate(unique_clusters)]\n",
    "    \n",
    "    # Create the plot\n",
    "    p = figure(title=\"UMAP projection of the HDBSCAN clusters\",\n",
    "               tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n",
    "               tooltips=\"@title\",\n",
    "               width=800, height=600)\n",
    "\n",
    "    legend_items = []\n",
    "    for cluster, color in zip(unique_clusters, colors):\n",
    "        cluster_data = clustered_df[clustered_df['cluster'] == cluster]\n",
    "        cluster_source = ColumnDataSource(data=dict(\n",
    "            x=for_visual_umap_embeddings[clustered_df['cluster'] == cluster, 0],\n",
    "            y=for_visual_umap_embeddings[clustered_df['cluster'] == cluster, 1],\n",
    "            title=cluster_data['title']\n",
    "        ))\n",
    "        \n",
    "        renderer = p.circle('x', 'y', size=10, source=cluster_source, color=color, fill_alpha=0.6, line_color=None)\n",
    "        legend_items.append((str(cluster), [renderer]))\n",
    "\n",
    "    # Add query point with triangle marker in bright pink\n",
    "    p.triangle(query_umap_embedding[0], query_umap_embedding[1], size=15, color='deeppink', \n",
    "              fill_alpha=0.8, line_color='deeppink', legend_label='Query')\n",
    "\n",
    "    # Add the legend\n",
    "    legend = Legend(items=legend_items, location=\"center\")\n",
    "    p.add_layout(legend, 'right')\n",
    "\n",
    "    # Customize the plot\n",
    "    p.xaxis.axis_label = 'UMAP Dimension 1'\n",
    "    p.yaxis.axis_label = 'UMAP Dimension 2'\n",
    "\n",
    "    # Show the plot\n",
    "    output_notebook()\n",
    "    show(p, notebook_handle=True)\n",
    "\n",
    "def plot_word_clouds(clustered_df, query):\n",
    "    # Create a figure to hold all the word clouds\n",
    "    num_clusters = len(clustered_df['cluster'].unique())\n",
    "    ncols = 3\n",
    "    nrows = (num_clusters // ncols) + (num_clusters % ncols > 0)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate through each cluster and generate a word cloud\n",
    "    for i, cluster in enumerate(sorted(clustered_df['cluster'].unique())):\n",
    "        cluster_data = clustered_df[clustered_df['cluster'] == cluster]\n",
    "        titles_text = \" \".join(cluster_data['title'].dropna())\n",
    "\n",
    "        # Remove the query from titles_text (case insensitive)\n",
    "        titles_text = re.sub(query, '', titles_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Generate the word cloud\n",
    "        wordcloud = WordCloud(width=400, height=200, background_color='white').generate(titles_text)\n",
    "        \n",
    "        # Display the word cloud\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Cluster {cluster}')\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(df, vector_embedding_model, query, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of all vectors with cosine(angle) > threshold with the embedded query\n",
    "    \"\"\"\n",
    "\n",
    "    if query == '':\n",
    "        # If the query is empty, return the original DataFrame\n",
    "        print('Returning all results')\n",
    "        return df\n",
    "\n",
    "    # Embed the query in transformer vector space (768 dimensions)\n",
    "    vector = vector_embedding_model.encode(query)\n",
    "\n",
    "    # Calculate the cosine similarity between the input vector and the vectors in the DataFrame\n",
    "    similarities = np.dot(df['embedding_llm_768_dim'].tolist(), vector)\n",
    "    norm_product = np.linalg.norm(df['embedding_llm_768_dim'].tolist(), axis=1) * np.linalg.norm(vector)\n",
    "    cosines = similarities / norm_product\n",
    "\n",
    "    # Add cosines to the DataFrame\n",
    "    df['cosine'] = cosines\n",
    "\n",
    "    # Filter the DataFrame by cosine similarity\n",
    "    filtered_df = df[cosines > threshold]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def umap_reduce(df):\n",
    "    \"\"\"\n",
    "    creates a umap manifold representing only the queried vectors\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd \n",
    "    import numpy as np\n",
    "    import math \n",
    "    import umap\n",
    "\n",
    "    # df_nn_10_md_0_nc_50 was the chosen one\n",
    "    \n",
    "    # note if too few results are returned, then n_neighbors=10 is too much to create a manifold\n",
    "    try:\n",
    "        umap_reducer = umap.UMAP(n_neighbors=10, min_dist=0, n_components=50, random_state=42)\n",
    "        umap_embeddings = umap_reducer.fit_transform(df['embedding_llm_768_dim'].tolist())\n",
    "        df['embedding_umap'] = list(umap_embeddings)\n",
    "    except Exception as e:\n",
    "        # print('Too few vectors, cannot create a manifold')\n",
    "        # print this later lol\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "def hdbscan_cluster(filtered_df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with cluster labels using the HDBSCAN clustering algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    if len(filtered_df) == 0:\n",
    "        print('No papers found')\n",
    "        return None\n",
    "\n",
    "    # determine min cluster size based on the number of papers\n",
    "    if len(filtered_df) < 20:\n",
    "        min_cluster_size = 2\n",
    "    else:\n",
    "        min_cluster_size = math.ceil(len(filtered_df) / 20)\n",
    "\n",
    "    embeddings = filtered_df['embedding_umap'].tolist()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                                    min_samples=5,\n",
    "                                    cluster_selection_epsilon=0.5,\n",
    "                                    metric='euclidean', \n",
    "                                    cluster_selection_method='eom', \n",
    "                                    allow_single_cluster=False,\n",
    "                                    gen_min_span_tree=True)\n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    filtered_df['cluster'] = cluster_labels\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def plot_cluster_word_frequencies(clustered_df):\n",
    "    def get_word_freq(text):\n",
    "        words = re.findall(r'\\w+', text.lower())        \n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        return Counter(words)\n",
    "\n",
    "    def get_author_freq(authors_lists):\n",
    "        # Flatten the list of author lists and count frequencies\n",
    "        all_authors = [author for sublist in authors_lists for author in sublist]\n",
    "        return Counter(all_authors)\n",
    "\n",
    "    clusters = sorted(clustered_df['cluster'].unique())\n",
    "    n_clusters = len(clusters)\n",
    "\n",
    "    if n_clusters > 0:\n",
    "        fig, axs = plt.subplots(n_clusters, 2, figsize=(24, 6*n_clusters))\n",
    "        if n_clusters == 1:\n",
    "            axs = axs.reshape(1, 2)\n",
    "        \n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            cluster_docs = clustered_df[clustered_df['cluster'] == cluster]\n",
    "            \n",
    "            # Word frequencies plot\n",
    "            combined_text = ' '.join(cluster_docs['title'] + ' ' + cluster_docs['abstract'])\n",
    "            word_freq = get_word_freq(combined_text)\n",
    "            top_words = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "            \n",
    "            # Reverse the order for vertical display\n",
    "            keys = list(reversed(list(top_words.keys())))\n",
    "            values = list(reversed(list(top_words.values())))\n",
    "            axs[idx, 0].barh(range(len(keys)), values)\n",
    "            axs[idx, 0].set_yticks(range(len(keys)))\n",
    "            axs[idx, 0].set_yticklabels(keys)\n",
    "            axs[idx, 0].set_title(f'Cluster {cluster}: Top 10 Words', fontsize=14)\n",
    "            axs[idx, 0].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "            # Author frequencies plot\n",
    "            author_freq = get_author_freq(cluster_docs['authors'])\n",
    "            top_authors = dict(sorted(author_freq.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "            \n",
    "            # Reverse the order for vertical display\n",
    "            keys = list(reversed(list(top_authors.keys())))\n",
    "            values = list(reversed(list(top_authors.values())))\n",
    "            axs[idx, 1].barh(range(len(keys)), values)\n",
    "            axs[idx, 1].set_yticks(range(len(keys)))\n",
    "            axs[idx, 1].set_yticklabels(keys)\n",
    "            axs[idx, 1].set_title(f'Cluster {cluster}: Top 10 Authors', fontsize=14)\n",
    "            axs[idx, 1].tick_params(axis='both', labelsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No clusters found in the data\")\n",
    "\n",
    "def cluster_from_query(df, vector_embedding_model, query, cosine_threshold, extra_visuals):\n",
    "    \"\"\"\n",
    "    Given a query and a DataFrame containing embedding_llm_768_dim, embedding_umap columns, returns a DataFrame with cluster labels\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_df = vector_search(df, vector_embedding_model, query, cosine_threshold)\n",
    "    \n",
    "    if len(filtered_df) == 0:\n",
    "        print('No papers found')\n",
    "        return None\n",
    "    \n",
    "    reduced_df = umap_reduce(filtered_df)\n",
    "\n",
    "    if reduced_df is None:\n",
    "        print('Too few vectors to run clustering algorithm')\n",
    "        print('Here are the papers found:')\n",
    "        display(filtered_df[['title', 'authors']])\n",
    "        filtered_df['cluster'] = -1\n",
    "        plot_cluster_word_frequencies(filtered_df)\n",
    "        return None\n",
    "\n",
    "    clustered_df = hdbscan_cluster(reduced_df)\n",
    "\n",
    "    plot_cluster_word_frequencies(clustered_df)\n",
    "\n",
    "    if extra_visuals:\n",
    "        umap_visualization(clustered_df, query, vector_embedding_model)\n",
    "        plot_word_clouds(clustered_df, query)\n",
    "    \n",
    "    return clustered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3e25ff26794d2cb32047d2d73cc025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <b>This tool allows you to search for a topic and view the relevant papers and project pages…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b27b6a18d4758b92841c79e046f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Search:', placeholder='Enter search query'), Label(value='         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bae8728ee541ada26726d2d936b061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "out = widgets.Output()\n",
    "# Define the function to run the code\n",
    "\n",
    "def run_cluster_and_visualize(query, cosine_threshold, extra_visuals):\n",
    "    out.clear_output(wait=True)  # Clear the output widget, not the whole cell\n",
    "    with out:\n",
    "        # Any print or output from cluster_from_query will be captured here\n",
    "        cluster_from_query(df, vector_embedding_model, query, cosine_threshold, extra_visuals)\n",
    "\n",
    "# Create a description of the tool\n",
    "tool_label = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <b>This tool allows you to search for a topic and view the relevant papers and project pages within SINTEF, categorized into subgroups.<br>\n",
    "    The cosine similarity threshold controls how close the results are to the query, 0.2 works well for general queries, and 0.4 for specific concepts.\n",
    "    </b>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a search bar\n",
    "search_bar = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter search query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a value slider for cosine threshold selection\n",
    "threshold_slider = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.2,\n",
    "    max=0.5,\n",
    "    step=0.01,\n",
    "    description='Threshold:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "# Create a checkbox\n",
    "toggle_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Extra cluster visuals',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a button\n",
    "search_button = widgets.Button(\n",
    "    description='Run',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to run the search',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "# Define the button click event\n",
    "def on_button_click(b):\n",
    "    run_cluster_and_visualize(search_bar.value, threshold_slider.value, toggle_checkbox.value)\n",
    "\n",
    "# Attach the button click event to the function\n",
    "search_button.on_click(on_button_click)\n",
    "\n",
    "# Create an HBox layout to place the search bar, slider, and button horizontally\n",
    "hbox = widgets.HBox([search_bar, widgets.Label(\" \" * 20), threshold_slider, widgets.Label(\" \" * 2), toggle_checkbox, widgets.Label(\" \" * 20), search_button])\n",
    "\n",
    "# Display the HBox layout and the button\n",
    "display(tool_label, hbox, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_staff_project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
